{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose \n",
    "\n",
    "### 商品・日付・店舗区分から販売数を予測\n",
    "* * *\n",
    "\n",
    "# Agenda\n",
    "\n",
    "1. 商品情報を説明変数とする\n",
    "1. 日付×店舗区分から、店舗毎の販売数を当てる時系列分析\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"http://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"e6907449-14da-4a20-8e32-6be2cd2305aa\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(global) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  if (typeof (window._bokeh_onload_callbacks) === \"undefined\") {\n",
       "    window._bokeh_onload_callbacks = [];\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    window._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    delete window._bokeh_onload_callbacks\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    window._bokeh_onload_callbacks.push(callback);\n",
       "    if (window._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    window._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        window._bokeh_is_loading--;\n",
       "        if (window._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };\n",
       "\n",
       "  var js_urls = ['https://cdn.pydata.org/bokeh/release/bokeh-0.12.0.min.js', 'https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.0.min.js', 'https://cdn.pydata.org/bokeh/release/bokeh-compiler-0.12.0.min.js'];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      Bokeh.$(\"#e6907449-14da-4a20-8e32-6be2cd2305aa\").text(\"BokehJS successfully loaded\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.0.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.0.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.0.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.0.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    for (var i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i](window.Bokeh);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  if (window._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(this));"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "% matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import bokeh.plotting as bplt\n",
    "from IPython.display import display, HTML\n",
    "from pprintpp import pprint as pp\n",
    "import xgboost as xgb\n",
    "from sklearn import cross_validation\n",
    "\n",
    "bplt.output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datanames  = ['product', 'sales', 'store', 'test', 'train']\n",
    "sex = ['m', 'w']\n",
    "age = ['00_19', '20_49', '50_']\n",
    "segments = ['m00_19', 'm20_49', 'm50_', 'w00_19', 'w20_49', 'w50_']\n",
    "pids = ['p000', 'p001', 'p002', 'p003', 'p004', 'p005', 'p006', 'p007',\n",
    "       'p008', 'p009', 'p010', 'p011', 'p012', 'p013', 'p014', 'p015',\n",
    "       'p016', 'p017', 'p018', 'p019', 'p020', 'p021', 'p022', 'p023',\n",
    "       'p024', 'p025', 'p026', 'p027', 'p028', 'p029', 'p030', 'p031',\n",
    "       'p032', 'p033', 'p034', 'p035', 'p036', 'p037', 'p038', 'p039',\n",
    "       'p040', 'p041', 'p042', 'p043', 'p044', 'p045', 'p046', 'p047',\n",
    "       'p048', 'p049', 'p050', 'p051', 'p052', 'p053', 'p054', 'p055',\n",
    "       'p056', 'p057', 'p058', 'p059', 'p060', 'p061', 'p062', 'p063',\n",
    "       'p064', 'p065', 'p066', 'p067', 'p068', 'p069', 'p070', 'p071',\n",
    "       'p072']\n",
    "train_dates = [201506, 201507, 201508, 201509, 201510, 201511, 201512, 201601, 201602, 201603, 201604, 201605]\n",
    "store_catecol = ['area', 'location']\n",
    "size_dict = {'size_g': 'g', 'size_stick':'本', 'size_piece':'個', 'size_sheet':'枚'}\n",
    "# 本 -> stick for index[50] , 個 -> stick for index[14, 15, 25, 37] , 枚 -> stick for index[27, 28] \n",
    "product_catecol = ['category', 'package_type', 'genre', 'manufacturer']\n",
    "product_apeals = ['cal', '食物繊維', '乳酸菌', 'オリーブオイル', '砂糖', '糖類', '糖質', '食塩']\n",
    "allergys= ['allergy_egg', 'allergy_wheat', 'allergy_milk', 'allergy_peanut', 'allergy_shrimp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_dict_origin = {datanames[n] : pd.read_csv('../data/{}.tsv'.format(name), delimiter='\\t') for n, name in enumerate(datanames)}\n",
    "df_dict = {name : df_dict_origin[name].copy() for name in df_dict_origin}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-paration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### store's categorical variables -> dummy\n",
    "\n",
    "* store_catecol = ['area', 'location']のダミー変数を生成\n",
    "* store_catecol = ['area', 'location']をドロップ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for df_n in ['train', 'test', 'store']:\n",
    "    for col in store_catecol:\n",
    "        dummy = pd.get_dummies(df_dict[df_n][col])\n",
    "        df_dict[df_n] = pd.concat((df_dict[df_n], dummy), axis=1)\n",
    "    df_dict[df_n] = df_dict[df_n].drop(store_catecol, axis =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 商品のサイズ種類\n",
    "\n",
    "* [g, 枚, 個, 本]にカテゴライズ\n",
    "* 対応するものにfloatで保管\n",
    "* 違うカラムには０を保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14     4個\n",
       "15     4個\n",
       "25     7個\n",
       "27     6枚\n",
       "28     5枚\n",
       "37     7個\n",
       "50    10本\n",
       "Name: size, dtype: object"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_n = 'product'\n",
    "col_n = 'size'\n",
    "entity = 'g'\n",
    "\n",
    "df_dict[df_n][df_dict[df_n][col_n].str.contains(entity).apply(lambda x: False if x else True)][col_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_n = 'product'\n",
    "col_n = 'size'\n",
    "decode_n = 'utf-8'\n",
    "dtype_d = 'float64'\n",
    "\n",
    "for size_n in size_dict:\n",
    "    df_dict[df_n][size_n] = df_dict[df_n][col_n].apply(lambda x: x.decode(decode_n)[:-1]).astype(dtype_d)\n",
    "    df_dict[df_n].loc[df_dict[df_n][col_n].str.contains(size_dict[size_n]).apply(lambda x: False if x else True), size_n] = 0\n",
    "df_dict[df_n] = df_dict[df_n].drop(col_n, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 商品のカテゴリカル変数 -> ダミー変数\n",
    "\n",
    "* category :  [おやつごろシリーズ菓子, ナチュラルローソン菓子]\n",
    "* package_type : [カップ, 箱, 袋]\n",
    "* genre : (total 24)[u'クッキー', u'クラッカー', u'スナック菓子', u'チョコレート', u'チョコレート菓子', u'ドーナツ', u'パイ', u'パイ加工品', u'ビスケット', u'ポテトチップス', u'マシュマロ', u'ラスク', u'半生菓子', u'有機干し芋', u'油菓子', u'準チョコレート', u'焼菓子', u'種実加工品', u'米菓', u'米菓子', u'菓子', u'野菜チップス', u'魚介乾製品', u'魚介加工品']\n",
    "* manufacturer : (total 40) [u'ぼんち株式会社', u'イケダヤ製菓株式会社', u'イトウ製菓株式会社', u'カバヤ食品株式会社', u'カルビー株式会社',u'ジャパンフリトレー株式会社', u'ダイシンフーズ株式会社', u'ニッポー株式会社', u'ハース株式会社', u'ホンダ製菓株式会社',u'リスカ株式会社', u'ローヤル製菓株式会社', u'久保田製菓有限会社', u'宝製菓株式会社', u'旺旺ジャパン株式会社',u'東京カリント株式会社', u'株式会社おやつカンパニー', u'株式会社エイワ', u'株式会社オリーヴ', u'株式会社カレイナ',u'株式会社ギンビス', u'株式会社ニッコー', u'株式会社ミツヤ', u'株式会社リボン', u'株式会社ロッテ',u'株式会社不二家', u'株式会社北食', u'株式会社合食', u'株式会社壮関', u'株式会社天乃屋', u'株式会社末広製菓',u'株式会社村田製菓', u'株式会社東ハト', u'株式会社栗山米菓', u'株式会社正栄デリシィ', u'株式会社湖池屋',u'株式会社道南冷蔵', u'森永製菓株式会社', u'江崎グリコ株式会社', u'阿部幸製菓株式会社']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_n = 'product'\n",
    "for col in product_catecol:\n",
    "    dummy = pd.get_dummies(df_dict[df_n][col])\n",
    "    df_dict[df_n] = pd.concat((df_dict[df_n], dummy), axis=1)\n",
    "df_dict[df_n] = df_dict[df_n].drop(product_catecol, axis =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 商品のアピールポイント -> ダミー変数\n",
    "\n",
    "* [cal, 食物繊維, 乳酸菌, オリーブオイル, 砂糖, 糖類, 糖質, 食塩]\n",
    "* アピールポイントに上記のワードを含む商品にフラグをたてる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          126kcal\n",
       "1         食物繊維3.8g\n",
       "2           63kcal\n",
       "3           61kcal\n",
       "4       乳酸菌200億個配合\n",
       "5        オリーブオイル使用\n",
       "6          130kcal\n",
       "7         食物繊維4.4g\n",
       "8        砂糖ゼロ　糖類ゼロ\n",
       "9        砂糖ゼロ・糖類ゼロ\n",
       "10        食物繊維2.1g\n",
       "11    乳酸菌2億個（5枚当り）\n",
       "12    乳酸菌2億個（5枚当り）\n",
       "13        食物繊維2.1g\n",
       "14          糖質7.5g\n",
       "15          糖質7.5g\n",
       "16         174kcal\n",
       "17         砂糖食塩不使用\n",
       "18        食物繊維3.6g\n",
       "19           砂糖不使用\n",
       "20          乳酸菌2億個\n",
       "21         168kcal\n",
       "22        食物繊維2.5g\n",
       "23        食物繊維5.2ｇ\n",
       "24         砂糖食塩不使用\n",
       "25          糖質9.3g\n",
       "Name: appeal_point, dtype: object"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dict[df_n].appeal_point[df_dict[df_n].appeal_point.apply(pd.notnull)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_n = 'product'\n",
    "col_n = 'appeal_point'\n",
    "\n",
    "df_dict[df_n][col_n] = df_dict[df_n][col_n].fillna(value='tmp')\n",
    "for word in product_apeals:\n",
    "    df_dict[df_n][word] = df_dict[df_n].appeal_point.apply(lambda x: 1 if word in x else 0)\n",
    "df_dict[df_n] = df_dict[df_n].drop(col_n, axis =1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### アレルギー情報の0補間"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_n = 'product'\n",
    "\n",
    "for col_n in allergys:\n",
    "    df_dict[df_n][col_n] = df_dict[df_n][col_n].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 説明文の有無フラグ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_n = 'product'\n",
    "col_n = 'description'\n",
    "\n",
    "df_dict[df_n][col_n] =  df_dict[df_n][col_n].apply(lambda x: 0 if pd.isnull(x) else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 原材料のダミー変数\n",
    "\n",
    "* エンコードがうまくいかない\n",
    "* カラム数が膨大\n",
    "* 以上の理由より一時放置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    73\n",
       "Name: materials, dtype: int64"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.isnull(df_dict['product'].materials).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    u'クルミ',\n",
      "    u'ココナッツ',\n",
      "    u'ア\\u30fcモンド',\n",
      "    u'グラニュ\\u30fc糖',\n",
      "    u'砂糖',\n",
      "    u'ショ糖',\n",
      "    u'でん粉',\n",
      "    u'ぶどう糖',\n",
      "    u'デキストリン',\n",
      "    u'食塩',\n",
      "    u'乳等を主要成分とする食品',\n",
      "    u'香料',\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "pp([unicode(word, 'utf-8') for word in df_dict['product'].materials[1].split('、')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 栄養成分量\n",
    "\n",
    "* 欠損値は0補間\n",
    "* 範囲表示のものは中間の値を使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_n = 'product'\n",
    "_split = '\\xef\\xbd\\x9e'\n",
    "_type = np.float\n",
    "colkey = 'nutrition'\n",
    "type_key = 'object'\n",
    "\n",
    "for col in df_dict[df_n].columns:\n",
    "    if colkey in col:\n",
    "        df_dict[df_n][col] = df_dict[df_n][col].fillna(value=0)\n",
    "        if df_dict[df_n][col].dtype == type_key:\n",
    "            _TrueFalse = df_dict[df_n][col].str.contains(_split).fillna(value=False)\n",
    "            inds = df_dict[df_n][_TrueFalse][col].index\n",
    "            for i in inds:\n",
    "                df_dict[df_n].loc[i, col] = np.array(df_dict[df_n][col][i].split(_split)).astype(np.float).mean()\n",
    "            df_dict[df_n][col] = df_dict[df_n][col].astype(_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 概要文と原材料をドロップ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "drop_col = ['abstract', 'materials', 'pname']\n",
    "df_n = 'product'\n",
    "\n",
    "df_dict[df_n] = df_dict[df_n].drop(drop_col, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## typeがintやfloatになっているのか確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######product######\n",
      "['pid', dtype('O')]\n",
      "#######sales######\n",
      "['pid', dtype('O')]\n",
      "['sid', dtype('O')]\n",
      "['segment', dtype('O')]\n",
      "#######store######\n",
      "['sid', dtype('O')]\n",
      "#######test######\n",
      "['pid', dtype('O')]\n",
      "#######train######\n",
      "['pid', dtype('O')]\n"
     ]
    }
   ],
   "source": [
    "for df_n in datanames:\n",
    "    print '#######{}######'.format(df_n)\n",
    "    for n, t in enumerate(df_dict[df_n].dtypes):\n",
    "        if t == 'object':\n",
    "            print [df_dict[df_n].columns[n], df_dict[df_n].dtypes[n]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pidでproduct_dfと[train_df, test_df]をマージ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pid\n",
      "pid\n"
     ]
    }
   ],
   "source": [
    "main_dfs = ['train', 'test']\n",
    "subdf_dict = {'pid': 'product'}\n",
    "_how = 'left'\n",
    "\n",
    "for df_n in main_dfs:\n",
    "    for _id in subdf_dict:\n",
    "        df_dict[df_n] = df_dict[df_n].merge(df_dict[subdf_dict[_id]], how=_how, on=_id).drop(_id, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## クロスバリデーション"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_n = 'train'\n",
    "label = 'y'\n",
    "train_X = df_dict[df_n].drop(label, axis=1).values\n",
    "train_Y = np.log(1+df_dict[df_n][label]).values[:, np.newaxis]\n",
    "\n",
    "df_n = 'test'\n",
    "test_X = df_dict[df_n].values.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "skf = cross_validation.KFold(train_Y.size, n_folds=10, shuffle=True,random_state=25)\n",
    "\n",
    "for train, test in skf:\n",
    "    X_Train, X_Test, y_Train, y_Test = train_X[train], train_X[test], train_Y[train], train_Y[test]\n",
    "    dtrain = xgb.DMatrix(X_Train, label=y_Train)\n",
    "    dvalid = xgb.DMatrix(X_Test, label=y_Test)\n",
    "    watchlist = [(dtrain, 'train'),(dvalid, 'eval')]\n",
    "    model = xgb.train(params, dtrain, num_boost_round=10,evals=watchlist,early_stopping_rounds=10)\n",
    "    predictions = model.predict(dvalid)\n",
    "    N = model.best_iteration\n",
    "    N_boost_round.append(N)\n",
    "    score = model.best_score\n",
    "    Score.append(score)\n",
    "Average_best_num_boost_round = np.average(N_boost_round)\n",
    "Average_best_score = np.average(Score)\n",
    "print \"\\tAverage of best iteration {0}\\n\".format(Average_best_num_boost_round)\n",
    "print \"\\tScore {0}\\n\\n\".format(Average_best_score)\n",
    "# return {'loss': Average_best_score, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = params={'max_depth': [5],\n",
    "        'subsample': [0.95],\n",
    "        'colsample_bytree': [1.0]\n",
    "}\n",
    "Score=[]\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "xgb_model = xgb.XGBRegressor()\n",
    "\n",
    "xgb_model = xgb.XGBClassifier()\n",
    "gs = GridSearchCV(xgb_model,\n",
    "                  params,\n",
    "                  cv=10,\n",
    "                  scoring='mean_squared_error',\n",
    "                  n_jobs=1,\n",
    "                  verbose=2)\n",
    "gs.fit(train_X,train_Y)\n",
    "predict = gs.predict(testX)\n",
    "\n",
    "print confusion_matrix(testY, predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "def rmsle(predicted, actual):\n",
    "    return np.sqrt(np.nansum(np.square(np.log(predicted + 1) - np.log(actual + 1)))/float(len(actual)))\n",
    "scorer = make_scorer(rmsle, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        2.0751\n",
       "1        1.9033\n",
       "2        1.9302\n",
       "3        1.7969\n",
       "4        1.5835\n",
       "5        1.6490\n",
       "6        1.5663\n",
       "7        1.7244\n",
       "8        1.8074\n",
       "9        1.9184\n",
       "10       1.6520\n",
       "11       2.0811\n",
       "12       3.7353\n",
       "13       3.6531\n",
       "14       3.5241\n",
       "15       3.2409\n",
       "16       3.2480\n",
       "17       3.1812\n",
       "18       2.9443\n",
       "19       2.6898\n",
       "20       2.8634\n",
       "21       3.5881\n",
       "22       3.7661\n",
       "23       3.9610\n",
       "24       2.1136\n",
       "25       1.9892\n",
       "26       2.0667\n",
       "27       1.8409\n",
       "28       1.6484\n",
       "29       1.9582\n",
       "          ...  \n",
       "22806    6.6322\n",
       "22807    5.7511\n",
       "22808    5.3596\n",
       "22809    6.3630\n",
       "22810    6.1183\n",
       "22811    5.5799\n",
       "22812    1.3215\n",
       "22813    2.2026\n",
       "22814    2.4230\n",
       "22815    2.8633\n",
       "22816    2.4228\n",
       "22817    1.9824\n",
       "22818    2.2026\n",
       "22819    3.9646\n",
       "22820    3.3040\n",
       "22821    2.8633\n",
       "22822    3.3039\n",
       "22823    1.9823\n",
       "22824    3.5665\n",
       "22825    3.1311\n",
       "22826    2.7709\n",
       "22827    3.0660\n",
       "22828    3.4316\n",
       "22829    4.3963\n",
       "22830    4.7247\n",
       "22831    5.0227\n",
       "22832    4.7809\n",
       "22833    4.9258\n",
       "22834    4.6815\n",
       "22835    3.8845\n",
       "Name: y, dtype: float64"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dict[df_n][label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        2.0751\n",
       "1        1.9033\n",
       "2        1.9302\n",
       "3        1.7969\n",
       "4        1.5835\n",
       "5        1.6490\n",
       "6        1.5663\n",
       "7        1.7244\n",
       "8        1.8074\n",
       "9        1.9184\n",
       "10       1.6520\n",
       "11       2.0811\n",
       "12       3.7353\n",
       "13       3.6531\n",
       "14       3.5241\n",
       "15       3.2409\n",
       "16       3.2480\n",
       "17       3.1812\n",
       "18       2.9443\n",
       "19       2.6898\n",
       "20       2.8634\n",
       "21       3.5881\n",
       "22       3.7661\n",
       "23       3.9610\n",
       "24       2.1136\n",
       "25       1.9892\n",
       "26       2.0667\n",
       "27       1.8409\n",
       "28       1.6484\n",
       "29       1.9582\n",
       "          ...  \n",
       "22806    6.6322\n",
       "22807    5.7511\n",
       "22808    5.3596\n",
       "22809    6.3630\n",
       "22810    6.1183\n",
       "22811    5.5799\n",
       "22812    1.3215\n",
       "22813    2.2026\n",
       "22814    2.4230\n",
       "22815    2.8633\n",
       "22816    2.4228\n",
       "22817    1.9824\n",
       "22818    2.2026\n",
       "22819    3.9646\n",
       "22820    3.3040\n",
       "22821    2.8633\n",
       "22822    3.3039\n",
       "22823    1.9823\n",
       "22824    3.5665\n",
       "22825    3.1311\n",
       "22826    2.7709\n",
       "22827    3.0660\n",
       "22828    3.4316\n",
       "22829    4.3963\n",
       "22830    4.7247\n",
       "22831    5.0227\n",
       "22832    4.7809\n",
       "22833    4.9258\n",
       "22834    4.6815\n",
       "22835    3.8845\n",
       "Name: y, dtype: float64"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(np.log(df_dict[df_n][label]+1))-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
